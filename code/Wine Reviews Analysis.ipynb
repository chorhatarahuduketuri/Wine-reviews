{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Problem\n",
    "\n",
    "So the problem is 'how do I know if wine is good?'  \n",
    "Specifically, when in a shop looking for something to buy. \n",
    "\n",
    "There are a few ways I could determine this. \n",
    "\n",
    "# The Solution(s)?\n",
    "\n",
    "1. I could do what I did with the other wine related datasets and train a tree model to get which are the ~3-5 most important factors, and what ranges they should lie in. While I think this will work - and I intend to do it - it also tends to always end up focussing on only one particular ordered set of questions with the same answers; country A, variety B, price C, etc. which doesn't tell me anything about good wines from country F (and I assume that country F must have some good wine or other? \n",
    "\n",
    "2. I could use some sort of relatively simple model (e.g. linear regression) and try to determine from the coefficients what features are important, and what values are better, but attempts to determine feature relevance in that way usually fail, as coefficients do not correlate with feature importance. I could use some sort of feature selection to further reduce the number of features I have, and make the result easier to understand somehow? The effectiveness of that is unclear, but unlikely. Then again, [scikit-learn's `SelectFromModel`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html \"scikit-learn docs\") exists to do just that - automatically - so maybe that's worth a look. This still doesn't actually tell me what _values_ of features mean good wine though, so I probably won't do this. \n",
    "\n",
    "3. I could train an accurate, black box model that classifies wine, then put that model into a phone app, and use it to give me a likely score (or range of scores) when I manually enter the features of the wine I am looking at on the shelf. The more data I enter, the more accurate the score. This seems like an approach that will provide an actual, physical solution to my problem, but will require a lot more work. \n",
    "\n",
    "# The Plan\n",
    "\n",
    "The first thing I'm going to do is the easiest - solution 1: decision tree models.  \n",
    "Of course the _zeroth_ thing I'm going to do is load and preprocess the data. \n",
    "\n",
    "### Modifications from the EDA\n",
    "The EDA was more about getting to know the data, finding out what was _feasibly physically usable_, and seeing how it could be prepared, _without prejudice_ with respect to what will later turn out to be useful (otherwise my assumptions and biases could cause me to discard or inappropriately interpret data). Now I come to the actual task I must reassess from what I've learned, and make new decisions based on _what difference it makes to the objective_, in order to avoid failing to move towards the value I'm working to create. \n",
    "\n",
    "At this point I have 10 features: \n",
    "1. country\n",
    "2. price\n",
    "3. province\n",
    "4. region_1\n",
    "5. region_2\n",
    "6. taster_name\n",
    "7. taster_twitter_handle\n",
    "8. variety\n",
    "9. winery\n",
    "10. vintage\n",
    "\n",
    "I am trying to find a way to determine the best (or a decent choice of) wine to buy when standing in a shop. That means that the only information I have available to me is what's on the bottle - if I was the type to google it or if I had an interest in learning about wine theory, I wouldn't need to be doing this project - so I should only use features that represent data I can get from the bottle. \n",
    "\n",
    "6. ~~taster_name~~\n",
    "7. ~~taster_twitter_handle~~\n",
    "\n",
    "That means taster_name and taster_twitter_handle are out because - again - if I was going to put in effort to learn wine theory and read reviews, I wouldn't need to be putting my effort towards this project. \n",
    "\n",
    "The country is always available, as is the price (though I will have to be mindful to remember currency conversions when not in the USA). The province is usually available, as is the data that makes up region_1 and region_2. The variety, winery, and vintage are also usually available for all but the cheapest of wines (the mixes and generic 'red wine'). \n",
    "\n",
    "So I end up with 8 features, which I will now begin work with: \n",
    "1. country\n",
    "2. price\n",
    "3. province\n",
    "4. region_1\n",
    "5. region_2\n",
    "6. variety\n",
    "7. winery\n",
    "8. vintage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inital data loading \n",
    "data = pd.read_csv('../input/wine-reviews/winemag-data-130k-v2.csv', index_col=0)\n",
    "target_data = data['points']\n",
    "feature_data = data.drop('points', axis=1)\n",
    "\n",
    "# Creation of new feature 'vintage'\n",
    "titles = feature_data.title.copy(deep=True)\n",
    "years = titles.replace(r'.*((19|20)[0-9]{2}).*', r'\\1', regex=True).replace(r'[A-Za-z]|[\\D]|\\s|(?:(?<!\\d)\\d{1}(?!\\d))|(?:(?<!\\d)\\d{2}(?!\\d))|(?:(?<!\\d)\\d{3}(?!\\d))', '', regex=True)\n",
    "filtered_years = years.replace(r'(1503|1607|1821|1827|1847|1868|1872|1868|1872|1882|1887|2067)', '', regex=True)\n",
    "vintage = filtered_years\n",
    "\n",
    "# Removal of unusable features (I can't find the reviewers description \n",
    "# on the bottle, can I?) and addition of new feature 'vintage'\n",
    "raw_reduced_features = feature_data.drop(labels=['description', 'designation', 'title', 'taster_name', 'taster_twitter_handle'], axis=1)\n",
    "raw_reduced_features['vintage'] = vintage.copy(deep=False).replace('', '0').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out all the features so I can transform them individually, \n",
    "# and then put them back together into different datasets\n",
    "country = raw_reduced_features.country\n",
    "price = raw_reduced_features.price\n",
    "province = raw_reduced_features.province\n",
    "region_1 = raw_reduced_features.region_1\n",
    "region_2 = raw_reduced_features.region_2\n",
    "variety = raw_reduced_features.variety\n",
    "winery = raw_reduced_features.winery\n",
    "vintage = raw_reduced_features.vintage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country', 'price', 'province', 'region_1', 'region_2', 'variety',\n",
       "       'winery', 'vintage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_reduced_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows left with no null values after dropping region_2: 50292\n"
     ]
    }
   ],
   "source": [
    "reduced_features_non_null_data = raw_reduced_features.dropna(subset=['country', 'price', 'province', 'region_1', 'region_2', 'variety', 'winery'], axis='index', how='any').copy(deep=False)\n",
    "print('Number of rows left with no null values after dropping region_2: {}'.format(reduced_features_non_null_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out all the features so I can transform them individually, \n",
    "# and then put them back together into different datasets\n",
    "country_non_null = reduced_features_non_null_data.country\n",
    "price_non_null = reduced_features_non_null_data.price\n",
    "province_non_null = reduced_features_non_null_data.province\n",
    "region_1_non_null = reduced_features_non_null_data.region_1\n",
    "region_2_non_null = reduced_features_non_null_data.region_2\n",
    "variety_non_null = reduced_features_non_null_data.variety\n",
    "winery_non_null = reduced_features_non_null_data.winery\n",
    "vintage_non_null = reduced_features_non_null_data.vintage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Numerical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country\n",
    "# nulls\n",
    "label_encoder_country = LabelEncoder()\n",
    "label_encoder_country.fit(country.astype(str))\n",
    "encoded_countries = label_encoder_country.transform(country.astype(str))\n",
    "\n",
    "# non-nulls\n",
    "label_encoder_non_null_country = LabelEncoder()\n",
    "label_encoder_non_null_country.fit(country_non_null.astype(str))\n",
    "encoded_non_null_countries = label_encoder_non_null_country.transform(country_non_null.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# province\n",
    "# nulls\n",
    "label_encoder_province = LabelEncoder()\n",
    "label_encoder_province.fit(province.astype(str))\n",
    "encoded_provinces = label_encoder_province.transform(province.astype(str))\n",
    "\n",
    "# non-nulls\n",
    "label_encoder_non_null_province = LabelEncoder()\n",
    "label_encoder_non_null_province.fit(province_non_null.astype(str))\n",
    "encoded_non_null_provinces = label_encoder_non_null_province.transform(province_non_null.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region_1\n",
    "# nulls\n",
    "label_encoder_region_1 = LabelEncoder()\n",
    "label_encoder_region_1.fit(region_1.astype(str))\n",
    "encoded_region_1s = label_encoder_region_1.transform(region_1.astype(str))\n",
    "\n",
    "# non-nulls\n",
    "label_encoder_non_null_region_1 = LabelEncoder()\n",
    "label_encoder_non_null_region_1.fit(region_1_non_null.astype(str))\n",
    "encoded_non_null_region_1s = label_encoder_non_null_region_1.transform(region_1_non_null.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region_2\n",
    "# nulls\n",
    "label_encoder_region_2 = LabelEncoder()\n",
    "label_encoder_region_2.fit(region_2.astype(str))\n",
    "encoded_region_2s = label_encoder_region_2.transform(region_2.astype(str))\n",
    "\n",
    "# non-nulls\n",
    "label_encoder_non_null_region_2 = LabelEncoder()\n",
    "label_encoder_non_null_region_2.fit(region_2_non_null.astype(str))\n",
    "encoded_non_null_region_2s = label_encoder_non_null_region_2.transform(region_2_non_null.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variety\n",
    "# nulls\n",
    "label_encoder_variety = LabelEncoder()\n",
    "label_encoder_variety.fit(variety.astype(str))\n",
    "encoded_varieties = label_encoder_variety.transform(variety.astype(str))\n",
    "\n",
    "# non-nulls\n",
    "label_encoder_non_null_variety = LabelEncoder()\n",
    "label_encoder_non_null_variety.fit(variety_non_null.astype(str))\n",
    "encoded_non_null_varieties = label_encoder_non_null_variety.transform(variety_non_null.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# winery\n",
    "# nulls\n",
    "label_encoder_winery = LabelEncoder()\n",
    "label_encoder_winery.fit(winery.astype(str))\n",
    "encoded_wineries = label_encoder_winery.transform(winery.astype(str))\n",
    "\n",
    "# non-nulls\n",
    "label_encoder_non_null_winery = LabelEncoder()\n",
    "label_encoder_non_null_winery.fit(winery_non_null.astype(str))\n",
    "encoded_non_null_wineries = label_encoder_non_null_winery.transform(winery_non_null.astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country\n",
    "# nulls\n",
    "one_hot_country = OneHotEncoder(sparse=False)\n",
    "one_hot_country.fit(country.astype(str).values.reshape(-1,1))\n",
    "one_hot_countries = one_hot_country.transform(country.astype(str).values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region_2\n",
    "one_hot_region_2 = OneHotEncoder(sparse=False)\n",
    "one_hot_region_2.fit(region_2.astype(str).values.reshape(-1,1))\n",
    "one_hot_region_2s = one_hot_region_2.transform(region_2.astype(str).values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoded feature data that contains nulls\n",
    "feature_data_label_encoded = raw_reduced_features.copy(deep=False)\n",
    "feature_data_label_encoded['vintage'] = vintage\n",
    "\n",
    "feature_data_label_encoded.country = encoded_countries\n",
    "feature_data_label_encoded.province = encoded_provinces\n",
    "feature_data_label_encoded.region_1 = encoded_region_1s\n",
    "feature_data_label_encoded.region_2 = encoded_region_2s\n",
    "feature_data_label_encoded.variety = encoded_varieties\n",
    "feature_data_label_encoded.winery = encoded_wineries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoded feature data that does not contain nulls\n",
    "feature_data_label_encoded_non_null = reduced_features_non_null_data.copy(deep=False)\n",
    "feature_data_label_encoded_non_null['vintage'] = vintage\n",
    "\n",
    "feature_data_label_encoded_non_null.country = encoded_non_null_countries\n",
    "feature_data_label_encoded_non_null.province = encoded_non_null_provinces\n",
    "feature_data_label_encoded_non_null.region_1 = encoded_non_null_region_1s\n",
    "feature_data_label_encoded_non_null.region_2 = encoded_non_null_region_2s\n",
    "feature_data_label_encoded_non_null.variety = encoded_non_null_varieties\n",
    "feature_data_label_encoded_non_null.winery = encoded_non_null_wineries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed encoding feature data\n",
    "feature_data_mixed_encoding = raw_reduced_features.copy(deep=False)\n",
    "feature_data_mixed_encoding = feature_data_mixed_encoding.drop(labels=['country', 'region_2'], axis=1)\n",
    "feature_data_mixed_encoding['vintage'] = vintage\n",
    "\n",
    "# Label encoding\n",
    "feature_data_mixed_encoding.province = encoded_provinces\n",
    "feature_data_mixed_encoding.region_1 = encoded_region_1s\n",
    "feature_data_mixed_encoding.variety = encoded_varieties\n",
    "feature_data_mixed_encoding.winery = encoded_wineries\n",
    "\n",
    "# One hot encoding\n",
    "one_hot_countries_df = pd.DataFrame(data=one_hot_countries, columns=one_hot_country.categories_[0])\n",
    "one_hot_countries_df = one_hot_countries_df.rename(columns={\"nan\":\"country_nan\"})\n",
    "feature_data_mixed_encoding = pd.concat([feature_data_mixed_encoding, one_hot_countries_df], axis=1)\n",
    "\n",
    "one_hot_region_2s_df = pd.DataFrame(data=one_hot_region_2s, columns=one_hot_region_2.categories_[0])\n",
    "one_hot_region_2s_df = one_hot_region_2s_df.rename(columns={\"nan\":\"region_2_nan\"})\n",
    "feature_data_mixed_encoding = pd.concat([feature_data_mixed_encoding, one_hot_region_2s_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test/Train Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoded full data test train split\n",
    "X_train_label_encoded, X_test_label_encoded, y_train_label_encoded, y_test_label_encoded = train_test_split(feature_data_label_encoded, target_data, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoded non-null data test train split\n",
    "target_data_non_null = target_data[feature_data_label_encoded_non_null.index]\n",
    "X_train_label_encoded_non_null, X_test_label_encoded_non_null, y_train_label_encoded_non_null, y_test_label_encoded_non_null = train_test_split(feature_data_label_encoded_non_null, target_data_non_null, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed encoding full data test train split\n",
    "X_train_mixed_encoding, X_test_mixed_encoding, y_train_mixed_encoding, y_test_mixed_encoding = train_test_split(feature_data_mixed_encoding, target_data, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoded data standardisation\n",
    "label_encoded_standard_scaler = StandardScaler()\n",
    "label_encoded_standard_scaler.fit(X_train_label_encoded)\n",
    "\n",
    "X_train_label_encoded_standardised = label_encoded_standard_scaler.transform(X_train_label_encoded)\n",
    "X_test_label_encoded_standardised = label_encoded_standard_scaler.transform(X_test_label_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoded non-null data standardisation\n",
    "label_encoded_non_null_standard_scaler = StandardScaler()\n",
    "label_encoded_non_null_standard_scaler.fit(X_train_label_encoded_non_null)\n",
    "\n",
    "X_train_label_encoded_non_null_standardised = label_encoded_non_null_standard_scaler.transform(X_train_label_encoded_non_null)\n",
    "X_test_label_encoded_non_null_standardised = label_encoded_non_null_standard_scaler.transform(X_test_label_encoded_non_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed encoding full data standardisation\n",
    "mixed_encoding_standard_scaler = StandardScaler()\n",
    "mixed_encoding_standard_scaler.fit(X_train_mixed_encoding)\n",
    "\n",
    "X_train_mixed_encoding_standardised = mixed_encoding_standard_scaler.transform(X_train_mixed_encoding)\n",
    "X_test_mixed_encoding_standardised = mixed_encoding_standard_scaler.transform(X_test_mixed_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1\n",
    "Decision trees - let's see what they tell me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1203422591589915"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtr = DecisionTreeRegressor(max_depth=7, min_samples_split=500, max_features=1, random_state=4)\n",
    "\n",
    "dtr.fit(X_train_label_encoded_non_null, y_train_label_encoded_non_null)\n",
    "\n",
    "preds = dtr.predict(X_test_label_encoded_non_null)\n",
    "\n",
    "mae = mean_absolute_error(y_test_label_encoded_non_null, preds)\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['dot', '-Tpng', 'dtr_test.dot', '-o', 'dtr_test.png'], returncode=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_graphviz(dtr, out_file='dtr_test.dot', \n",
    "                feature_names=feature_data_label_encoded_non_null.columns, \n",
    "                label='all', \n",
    "                filled=True, \n",
    "                rounded=True,\n",
    "                leaves_parallel=True, \n",
    "                impurity=True)\n",
    "subprocess.run(['dot','-Tpng', 'dtr_test.dot', '-o', 'dtr_test.png'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dtr_test](dtr_test.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
